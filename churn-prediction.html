<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Beta Bank Churn Prediction ‚Äî Viet Nguyen</title>
  <link rel="stylesheet" href="style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet" />
  <style>
    /* ‚îÄ‚îÄ Article Page Styles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .article-hero {
      min-height: 320px;
      background: linear-gradient(135deg,
        #0d1b2a 0%,
        #112240 20%,
        #1a3a5c 45%,
        #0d7377 72%,
        #14a098 100%
      );
      display: flex;
      align-items: flex-end;
      padding: 5rem 1.5rem 2.75rem;
      position: relative;
      overflow: hidden;
    }

    .article-hero::before {
      content: '';
      position: absolute;
      inset: 0;
      background: radial-gradient(ellipse at 25% 60%, rgba(100,255,218,0.07) 0%, transparent 65%);
    }

    .article-hero-inner {
      max-width: 780px;
      margin: 0 auto;
      width: 100%;
      position: relative;
      z-index: 1;
    }

    .article-hero-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.78rem;
      color: var(--accent);
      letter-spacing: 0.1em;
      text-transform: uppercase;
      margin-bottom: 0.75rem;
      opacity: 0.9;
    }

    .article-hero-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(1.75rem, 4.5vw, 2.75rem);
      font-weight: 700;
      color: #ffffff;
      line-height: 1.2;
      margin-bottom: 1rem;
    }

    .article-hero-meta {
      font-size: 0.85rem;
      color: rgba(204, 214, 246, 0.55);
      font-family: 'JetBrains Mono', monospace;
    }

    /* ‚îÄ‚îÄ Article Body ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .article-body {
      max-width: 780px;
      margin: 0 auto;
      padding: 3rem 1.5rem 5rem;
    }

    .article-back {
      display: inline-flex;
      align-items: center;
      gap: 0.4rem;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.78rem;
      color: var(--accent);
      text-decoration: none;
      margin-bottom: 3rem;
      opacity: 0.65;
      transition: opacity 0.2s;
    }
    .article-back:hover { opacity: 1; color: var(--accent); }

    .article-section { margin-bottom: 3rem; }

    .article-h2 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 1.35rem;
      font-weight: 700;
      color: var(--accent);
      margin-bottom: 1rem;
      margin-top: 0;
    }

    .article-p {
      color: var(--text);
      line-height: 1.85;
      margin-bottom: 1rem;
      font-size: 1rem;
    }

    .article-p code {
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85em;
      color: var(--accent);
      background: rgba(100,255,218,0.07);
      padding: 0.1em 0.4em;
      border-radius: 4px;
    }

    .article-ul {
      color: var(--text);
      line-height: 1.85;
      padding-left: 1.5rem;
      margin-bottom: 1rem;
    }

    .article-ul li { margin-bottom: 0.65rem; }

    .article-divider {
      border: none;
      border-top: 1px solid rgba(100,255,218,0.1);
      margin: 2.75rem 0;
    }

    /* Callout box */
    .callout {
      background: rgba(100,255,218,0.04);
      border-left: 3px solid var(--accent);
      border-radius: 0 8px 8px 0;
      padding: 1.1rem 1.5rem;
      margin: 1.5rem 0;
      color: var(--text);
      font-style: italic;
      line-height: 1.75;
    }

    /* Phase blocks */
    .phase-block {
      margin-bottom: 2.25rem;
      padding-left: 1.25rem;
      border-left: 2px solid rgba(100,255,218,0.18);
    }

    .phase-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.73rem;
      color: var(--accent);
      letter-spacing: 0.1em;
      text-transform: uppercase;
      margin-bottom: 0.25rem;
      opacity: 0.8;
    }

    .phase-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 1rem;
      font-weight: 700;
      color: var(--accent);
      margin: 0 0 0.75rem;
    }

    /* Results tables */
    .result-table-wrap {
      overflow-x: auto;
      margin: 1.5rem 0;
      border-radius: 8px;
    }

    .result-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.875rem;
      background: rgba(10,25,47,0.85);
      border: 1px solid rgba(100,255,218,0.14);
      border-radius: 8px;
      overflow: hidden;
    }

    .result-table th {
      background: rgba(100,255,218,0.06);
      color: var(--accent);
      padding: 0.7rem 1rem;
      text-align: left;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.8rem;
      font-weight: 500;
      letter-spacing: 0.04em;
      border-bottom: 1px solid rgba(100,255,218,0.14);
    }

    .result-table td {
      padding: 0.65rem 1rem;
      color: var(--text);
      border-bottom: 1px solid rgba(100,255,218,0.06);
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.82rem;
    }

    .result-table tr:last-child td { border-bottom: none; }
    .result-table tr:hover td { background: rgba(100,255,218,0.02); }

    .td-label { font-family: 'Inter', sans-serif; font-size: 0.88rem; }

    .badge-pass  { color: #4ade80; font-weight: 600; }
    .badge-fail  { color: #f87171; }
    .badge-best  { color: var(--accent); font-weight: 600; }
    .badge-note  { color: var(--text-muted); font-size: 0.78rem; }

    /* CTA */
    .article-cta {
      text-align: center;
      padding: 3rem 1rem;
      border-top: 1px solid rgba(100,255,218,0.1);
      margin-top: 1rem;
    }

    .article-cta h3 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 1.4rem;
      color: #e6f1ff;
      margin-bottom: 0.6rem;
    }

    .article-cta p {
      color: var(--text-muted);
      margin-bottom: 1.75rem;
      font-size: 0.95rem;
    }

    .cta-buttons {
      display: flex;
      gap: 1rem;
      justify-content: center;
      flex-wrap: wrap;
    }

    strong { color: #e6f1ff; }
    em     { color: var(--text-muted); }

    /* ‚îÄ‚îÄ Code Blocks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .code-wrap {
      margin: 1.5rem 0;
      border-radius: 10px;
      overflow: hidden;
      border: 1px solid rgba(100,255,218,0.12);
    }

    .code-header {
      background: #1a2744;
      padding: 0.55rem 1rem;
      display: flex;
      align-items: center;
      gap: 0.4rem;
    }

    .code-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
    }
    .code-dot-r { background: #ff5f57; }
    .code-dot-y { background: #febc2e; }
    .code-dot-g { background: #28c840; }

    .code-block {
      background: #0d1829;
      padding: 1.25rem 1.5rem;
      margin: 0;
      overflow-x: auto;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.82rem;
      line-height: 1.75;
      color: #cdd6f4;
    }

    .code-block .kw  { color: #c792ea; }  /* keywords: import, from, def, if */
    .code-block .fn  { color: #82aaff; }  /* function / method names */
    .code-block .str { color: #c3e88d; }  /* strings */
    .code-block .num { color: #f78c6c; }  /* numbers */
    .code-block .cm  { color: #546e7a; font-style: italic; }  /* comments */
    .code-block .acc { color: #64ffda; }  /* accent ‚Äî class names, key params */

    /* Growth / learning callout */
    .growth-block {
      background: rgba(100,255,218,0.03);
      border: 1px solid rgba(100,255,218,0.12);
      border-radius: 10px;
      padding: 1.25rem 1.5rem;
      margin: 1.75rem 0;
    }
    .growth-block-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.72rem;
      color: var(--accent);
      letter-spacing: 0.1em;
      text-transform: uppercase;
      margin-bottom: 0.5rem;
      opacity: 0.8;
    }
    .growth-block p {
      color: var(--text);
      line-height: 1.8;
      margin: 0;
    }

    @media (max-width: 600px) {
      .article-hero { min-height: 260px; padding: 4.5rem 1rem 2rem; }
      .article-body { padding: 2rem 1rem 3rem; }
      .article-back { margin-bottom: 2rem; }
      .result-table { font-size: 0.78rem; }
      .result-table td, .result-table th { padding: 0.55rem 0.65rem; }
    }

    /* ‚îÄ‚îÄ Light Theme Overrides ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
       Replace hardcoded dark/neon values with light-mode colors  */
    [data-theme="light"] .article-p code {
      background: rgba(13,148,136,0.08);
    }
    [data-theme="light"] .article-divider {
      border-top-color: var(--border);
    }
    [data-theme="light"] .callout {
      background: rgba(13,148,136,0.05);
    }
    [data-theme="light"] .phase-block {
      border-left-color: rgba(13,148,136,0.4);
    }
    [data-theme="light"] .result-table {
      background: #ffffff;
      border-color: var(--border);
    }
    [data-theme="light"] .result-table th {
      background: rgba(13,148,136,0.06);
      border-bottom-color: var(--border);
    }
    [data-theme="light"] .result-table td {
      border-bottom-color: var(--border);
    }
    [data-theme="light"] .result-table tr:hover td {
      background: rgba(13,148,136,0.03);
    }
    [data-theme="light"] .article-cta {
      border-top-color: var(--border);
    }
    [data-theme="light"] .article-cta h3 {
      color: var(--text);
    }
    [data-theme="light"] strong {
      color: var(--text);
    }
    [data-theme="light"] .code-wrap {
      border-color: var(--border);
    }
    [data-theme="light"] .growth-block {
      background: rgba(13,148,136,0.04);
      border-color: rgba(13,148,136,0.18);
    }
  </style>
</head>
<body>

  <!-- ‚îÄ‚îÄ Tile Grid Cursor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <canvas id="tileCanvas"></canvas>

  <!-- ‚îÄ‚îÄ Preloader ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <div class="preloader" id="preloader">
    <div class="preloader-inner">
      <div class="preloader-logo">VN</div>
      <div class="preloader-bar"><div class="preloader-fill"></div></div>
    </div>
  </div>

  <!-- ‚îÄ‚îÄ Film Grain Overlay ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <div class="grain-overlay" aria-hidden="true"></div>

  <!-- ‚îÄ‚îÄ Easter Egg Modal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄÔøΩÔøΩÔøΩ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <div class="easter-egg" id="easterEgg" role="dialog" aria-modal="true">
    <div class="easter-egg-inner">
      <p class="easter-egg-title">üéâ Secret Unlocked!</p>
      <p>You found it! Viet's robotics teams qualified for 4 consecutive World Championships ‚Äî that's the real Easter egg. ‚Üë‚Üë‚Üì‚Üì‚Üê‚Üí‚Üê‚ÜíBA</p>
      <button onclick="document.getElementById('easterEgg').classList.remove('active')">Close</button>
    </div>
  </div>

  <!-- ‚îÄ‚îÄ Navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <nav id="navbar">
    <div class="nav-inner">
      <a class="nav-logo" href="index.html">VN</a>
      <button class="nav-toggle" id="navToggle" aria-label="Toggle menu">
        <span></span><span></span><span></span>
      </button>
      <ul class="nav-links" id="navLinks">
        <li><a href="index.html#about">About</a></li>
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#skills">Skills</a></li>
        <li><a href="index.html#experience">Experience</a></li>
        <li><a href="index.html#contact" class="nav-cta">Contact</a></li>
      </ul>
      <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme" title="Toggle light/dark mode">
        <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
        <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
      </button>
    </div>
  </nav>

  <!-- ‚îÄ‚îÄ Article Hero Banner ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <div class="article-hero">
    <div class="article-hero-inner">
      <p class="article-hero-label">TripleTen ¬∑ Sprint 9 ‚Äî Feature Engineering</p>
      <h1 class="article-hero-title">Predicting Who Leaves:<br>Beta Bank Customer Churn</h1>
      <p class="article-hero-meta">By Viet Nguyen &nbsp;¬∑&nbsp; Completed January 24, 2026 &nbsp;¬∑&nbsp; 8 min read</p>
    </div>
  </div>

  <!-- ‚îÄ‚îÄ Article Content ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <main class="article-body">

    <a href="index.html#projects" class="article-back">‚Üê Back to Projects</a>

    <!-- HOOK -->
    <div class="article-section reveal">
      <h2 class="article-h2">What Happens When a Customer Quietly Walks Out the Door?</h2>
      <p class="article-p">
        Banks lose customers every day. Not loudly ‚Äî there's no argument, no complaint letter, no dramatic
        exit. Customers just stop. They open an account somewhere else, move their direct deposit, and
        transfer their balance. By the time the bank notices the account has gone quiet, that customer
        is already gone.
      </p>
      <p class="article-p">
        For <strong>Beta Bank</strong>, this silent exodus was happening at a rate of <strong>20.4%</strong> ‚Äî
        roughly 1 in 5 customers had already churned. The question wasn't just <em>who left</em>, but
        <em>who's about to?</em> That's where machine learning comes in.
      </p>
      <div class="callout">
        If a model can flag at-risk customers before they leave, the bank has a window of opportunity ‚Äî
        a better interest rate, a personal call, a loyalty offer. That window is everything.
      </div>
    </div>

    <hr class="article-divider" />

    <!-- WHY THIS PROJECT -->
    <div class="article-section reveal">
      <h2 class="article-h2">Why This Project?</h2>
      <p class="article-p">
        This was Sprint 9 of my <strong>TripleTen AI/ML Engineer Bootcamp</strong>, focused on feature
        engineering and classification modeling. The project requirement was clear: achieve a minimum
        <strong>F1 score ‚â• 0.59</strong> on the held-out test set. But I treated it as a real
        business problem, not just a benchmark to clear.
      </p>
      <p class="article-p">
        Customer churn prediction is one of the most practical applications of machine learning in
        financial services. Every company with recurring customers faces it ‚Äî banks, telecom, SaaS,
        streaming. Getting good at solving it means understanding class imbalance, data leakage,
        and metric selection at a level that transfers across industries. That's the intuition
        worth building.
      </p>
    </div>

    <hr class="article-divider" />

    <!-- WHAT YOU'LL LEARN -->
    <div class="article-section reveal">
      <h2 class="article-h2">What You'll Learn from This</h2>
      <ul class="article-ul">
        <li>How to test whether missing data is random before imputing ‚Äî and why that test matters</li>
        <li>Why class imbalance turns a "79% accurate" model into a completely useless one</li>
        <li>The difference between three imbalance-correction strategies: class weights, upsampling, downsampling</li>
        <li>How GridSearchCV automates the search across 108 hyperparameter combinations</li>
        <li>Why F1 score ‚Äî not accuracy ‚Äî is the right metric when one class is rare</li>
      </ul>
    </div>

    <hr class="article-divider" />

    <!-- KEY TAKEAWAYS -->
    <div class="article-section reveal">
      <h2 class="article-h2">Key Takeaways</h2>
      <ul class="article-ul">
        <li><strong>Random Forest with balanced class weights</strong> was the best model ‚Äî F1 = 0.6381 on validation, <strong>0.6197 on the final test set</strong>, exceeding the ‚â• 0.59 target</li>
        <li>All three imbalance techniques passed the F1 target ‚Äî but class weights won without touching the training data</li>
        <li>9.1% of customers had missing Tenure data ‚Äî a Missing At Random (MAR) test confirmed median imputation was safe</li>
        <li>The model correctly identified <strong>~77% of actual churners</strong> in the test set (recall = 0.77) ‚Äî the retention team catches most of who matters</li>
        <li>ROC-AUC of <strong>0.8618</strong> confirms the model genuinely discriminates between churners and loyal customers</li>
      </ul>
    </div>

    <hr class="article-divider" />

    <!-- DATASET -->
    <div class="article-section reveal">
      <h2 class="article-h2">The Dataset</h2>
      <p class="article-p">
        Beta Bank's customer history: <strong>10,000 records, 14 columns</strong>. Features covered
        demographics (age, geography, gender), financial profile (credit score, balance, estimated salary),
        and behavioral signals (number of products, active membership status, tenure, credit card ownership).
        The target column ‚Äî <code>Exited</code> ‚Äî flagged whether a customer had already left.
      </p>
      <div class="result-table-wrap">
        <table class="result-table">
          <thead>
            <tr><th>Category</th><th>Detail</th></tr>
          </thead>
          <tbody>
            <tr><td class="td-label">Total records</td><td>10,000 customers</td></tr>
            <tr><td class="td-label">Features</td><td>13 (after dropping identifiers)</td></tr>
            <tr><td class="td-label">Target column</td><td><code>Exited</code> &nbsp;(0 = Stayed, 1 = Churned)</td></tr>
            <tr><td class="td-label">Class split</td><td>79.6% Stayed &nbsp;/&nbsp; 20.4% Churned</td></tr>
            <tr><td class="td-label">Missing values</td><td>909 rows missing Tenure (9.1%)</td></tr>
          </tbody>
        </table>
      </div>
      <div class="callout">
        A model that lazily predicted "not churned" for every customer would score 79.6% accuracy ‚Äî
        and catch zero churners. This is the class imbalance problem, and solving it was the
        core challenge of this project.
      </div>
    </div>

    <hr class="article-divider" />

    <!-- ANALYSIS PROCESS -->
    <div class="article-section reveal">
      <h2 class="article-h2">My Process</h2>

      <div class="phase-block">
        <p class="phase-label">Phase 1</p>
        <h3 class="phase-title">Import Libraries &amp; Load Data</h3>
        <p class="article-p">
          Loaded the dataset and configured the environment: pandas, NumPy, matplotlib, seaborn,
          and scikit-learn for modeling, preprocessing, and evaluation metrics.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block"><span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="kw">from</span> sklearn.model_selection  <span class="kw">import</span> train_test_split, GridSearchCV
<span class="kw">from</span> sklearn.preprocessing    <span class="kw">import</span> StandardScaler
<span class="kw">from</span> sklearn.linear_model    <span class="kw">import</span> LogisticRegression
<span class="kw">from</span> sklearn.tree             <span class="kw">import</span> DecisionTreeClassifier
<span class="kw">from</span> sklearn.ensemble         <span class="kw">import</span> RandomForestClassifier
<span class="kw">from</span> sklearn.metrics          <span class="kw">import</span> f1_score, roc_auc_score, classification_report
<span class="kw">from</span> sklearn.utils            <span class="kw">import</span> resample, shuffle

df = pd.<span class="fn">read_csv</span>(<span class="str">"/datasets/Churn.csv"</span>)</pre>
        </div>
      </div>

      <div class="phase-block">
        <p class="phase-label">Phase 2</p>
        <h3 class="phase-title">Exploratory Data Analysis</h3>
        <p class="article-p">
          First look at the data revealed the dataset's shape (10,000 √ó 14), 909 missing values in
          Tenure, and a 4:1 imbalance between stayed and churned customers. These three findings
          shaped every decision that followed.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block"><span class="fn">print</span>(<span class="str">"Shape:"</span>, df.shape)          <span class="cm"># (10000, 14)</span>
<span class="fn">print</span>(df.<span class="fn">isnull</span>().<span class="fn">sum</span>())           <span class="cm"># Tenure: 909 missing</span>

<span class="fn">print</span>(df[<span class="str">'Exited'</span>].<span class="fn">value_counts</span>(normalize=<span class="acc">True</span>))
<span class="cm"># 0    0.7963  (Stayed)</span>
<span class="cm"># 1    0.2037  (Churned)  ‚Üê significant class imbalance</span></pre>
        </div>
      </div>

      <div class="phase-block">
        <p class="phase-label">Phase 3</p>
        <h3 class="phase-title">Data Preparation</h3>
        <p class="article-p">
          Before filling those 909 missing Tenure values, I asked: <em>why are they missing?</em>
          If customers without Tenure data churn at a systematically different rate, the missingness
          is informative ‚Äî and imputation could distort the model's signal.
        </p>
        <p class="article-p">
          I ran a <strong>Missing At Random (MAR) test</strong> ‚Äî comparing churn rates between
          customers with and without Tenure data. The difference was less than 1%. The missingness
          was random, not systematic. Median imputation was safe. Here's why median specifically:
        </p>
        <ul class="article-ul">
          <li>Distribution is roughly uniform ‚Äî mean and median land close to each other</li>
          <li>Robust to outliers (customers with 10-year tenure pull the mean up)</li>
          <li>Missing values confirmed random ‚Äî less than 1% difference in churn rates between groups</li>
          <li>Only 9.1% of data affected ‚Äî minimal distortion to the overall dataset</li>
        </ul>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block"><span class="cm"># Missing At Random (MAR) test</span>
has_tenure    = df[<span class="str">'Tenure'</span>].notna()
churn_with    = df[has_tenure][<span class="str">'Exited'</span>].<span class="fn">mean</span>()
churn_without = df[~has_tenure][<span class="str">'Exited'</span>].<span class="fn">mean</span>()
<span class="cm"># Difference &lt; 5% ‚Üí missingness is random ‚Üí safe to impute</span>

df[<span class="str">'Tenure'</span>].<span class="fn">fillna</span>(df[<span class="str">'Tenure'</span>].<span class="fn">median</span>(), inplace=<span class="acc">True</span>)

<span class="cm"># Drop identifiers ‚Äî no predictive signal</span>
df_clean = df.<span class="fn">drop</span>([<span class="str">'RowNumber'</span>, <span class="str">'CustomerId'</span>, <span class="str">'Surname'</span>], axis=<span class="num">1</span>)

<span class="cm"># Encode categorical features</span>
df_clean[<span class="str">'Gender'</span>] = df_clean[<span class="str">'Gender'</span>].<span class="fn">map</span>({<span class="str">'Female'</span>: <span class="num">0</span>, <span class="str">'Male'</span>: <span class="num">1</span>})
df_clean = pd.<span class="fn">get_dummies</span>(df_clean, columns=[<span class="str">'Geography'</span>], drop_first=<span class="acc">True</span>)</pre>
        </div>
        <p class="article-p">
          I also removed three identifier columns ‚Äî Row Number, Customer ID, and Surname ‚Äî since
          they carry zero predictive signal. Then I encoded categorical features: binary mapping
          for Gender (Female=0, Male=1) and one-hot encoding for Geography (France, Germany, Spain).
        </p>
      </div>

      <div class="phase-block">
        <p class="phase-label">Phase 4</p>
        <h3 class="phase-title">Class Balance &amp; Baseline Models</h3>
        <p class="article-p">
          A <strong>stratified 60/20/20 split</strong> kept the 20.4% churn ratio consistent across
          train, validation, and test sets. After splitting, I verified: all three sets maintained
          the original 20.4% churn rate ‚Äî stratification successful. StandardScaler was fit only on
          the training set, then applied (never refitted) to validation and test ‚Äî fitting on all data
          would leak information about the test distribution into the model.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block">X = df_clean.<span class="fn">drop</span>(<span class="str">'Exited'</span>, axis=<span class="num">1</span>)
y = df_clean[<span class="str">'Exited'</span>]

<span class="cm"># Stratified 60 / 20 / 20 split</span>
X_temp, X_test, y_temp, y_test = <span class="fn">train_test_split</span>(
    X, y, test_size=<span class="num">0.2</span>, random_state=<span class="num">42</span>, stratify=y
)
X_train, X_valid, y_train, y_valid = <span class="fn">train_test_split</span>(
    X_temp, y_temp, test_size=<span class="num">0.25</span>, random_state=<span class="num">42</span>, stratify=y_temp
)

<span class="cm"># Scale ‚Äî fit on train only to prevent data leakage</span>
scaler = <span class="acc">StandardScaler</span>()
X_train_scaled = scaler.<span class="fn">fit_transform</span>(X_train)
X_valid_scaled = scaler.<span class="fn">transform</span>(X_valid)
X_test_scaled  = scaler.<span class="fn">transform</span>(X_test)</pre>
        </div>
        <p class="article-p">
          Then I trained three baseline models with no imbalance correction to establish a
          performance floor. The goal here wasn't to pass the target ‚Äî it was to understand where
          the models stood before any intervention:
        </p>
        <div class="result-table-wrap">
          <table class="result-table">
            <thead>
              <tr><th>Model</th><th>F1 Score</th><th>vs. Target (‚â• 0.59)</th></tr>
            </thead>
            <tbody>
              <tr><td class="td-label">Logistic Regression</td><td>0.3190</td><td class="badge-fail">Below target</td></tr>
              <tr><td class="td-label">Decision Tree</td><td>0.5573</td><td class="badge-fail">Below target</td></tr>
              <tr><td class="td-label">Random Forest</td><td>0.5573</td><td class="badge-fail">Below target</td></tr>
            </tbody>
          </table>
        </div>
        <p class="article-p">
          None of them hit F1 ‚â• 0.59. The models were biased toward predicting "stayed" because
          that was the safe, high-frequency answer. The imbalance needed to be corrected.
        </p>
      </div>

      <div class="phase-block">
        <p class="phase-label">Phase 5</p>
        <h3 class="phase-title">Imbalance Handling, Tuning &amp; Final Test</h3>
        <p class="article-p">
          I tested three imbalance-correction approaches on Random Forest ‚Äî the strongest baseline ‚Äî
          and compared their results on the validation set:
        </p>
        <p class="article-p">
          <strong>Approach 1 ‚Äî Class Weights:</strong> Set <code>class_weight='balanced'</code>
          in the model. This tells the algorithm to penalize misclassifying churners more heavily,
          without touching the data at all.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block">rf_weighted = <span class="acc">RandomForestClassifier</span>(
    random_state=<span class="num">42</span>,
    n_estimators=<span class="num">100</span>,
    max_depth=<span class="num">10</span>,
    class_weight=<span class="str">'balanced'</span>   <span class="cm"># ‚Üê key parameter</span>
)
rf_weighted.<span class="fn">fit</span>(X_train_scaled, y_train)

f1_weighted = <span class="fn">f1_score</span>(y_valid, rf_weighted.<span class="fn">predict</span>(X_valid_scaled))
<span class="cm"># f1_weighted ‚Üí 0.6381  ‚úì  exceeds target of 0.59</span></pre>
        </div>
        <p class="article-p">
          <strong>Approach 2 ‚Äî Upsampling (Oversampling):</strong> Duplicated minority class samples
          (with replacement) until both classes were equal in size ‚Äî roughly 9,600 training
          examples total.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block">X_df = pd.<span class="fn">DataFrame</span>(X_train_scaled, columns=X.columns)
X_df[<span class="str">'Exited'</span>] = y_train.values

majority = X_df[X_df[<span class="str">'Exited'</span>] == <span class="num">0</span>]
minority = X_df[X_df[<span class="str">'Exited'</span>] == <span class="num">1</span>]

minority_up = <span class="fn">resample</span>(minority, replace=<span class="acc">True</span>,
                        n_samples=<span class="fn">len</span>(majority), random_state=<span class="num">42</span>)
balanced    = <span class="fn">shuffle</span>(pd.<span class="fn">concat</span>([majority, minority_up]), random_state=<span class="num">42</span>)

rf_upsampled = <span class="acc">RandomForestClassifier</span>(random_state=<span class="num">42</span>, n_estimators=<span class="num">100</span>, max_depth=<span class="num">10</span>)
rf_upsampled.<span class="fn">fit</span>(balanced.<span class="fn">drop</span>(<span class="str">'Exited'</span>, axis=<span class="num">1</span>).values, balanced[<span class="str">'Exited'</span>].values)
<span class="cm"># f1_upsampled ‚Üí 0.6173  ‚úì</span></pre>
        </div>
        <p class="article-p">
          <strong>Approach 3 ‚Äî Downsampling (Undersampling):</strong> Randomly removed majority
          class samples until both classes matched ‚Äî roughly 2,400 training examples total,
          discarding a significant portion of the original data.
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block">majority_down = <span class="fn">resample</span>(majority, replace=<span class="acc">False</span>,
                          n_samples=<span class="fn">len</span>(minority), random_state=<span class="num">42</span>)
balanced_down = <span class="fn">shuffle</span>(pd.<span class="fn">concat</span>([majority_down, minority]), random_state=<span class="num">42</span>)

rf_downsampled = <span class="acc">RandomForestClassifier</span>(random_state=<span class="num">42</span>, n_estimators=<span class="num">100</span>, max_depth=<span class="num">10</span>)
rf_downsampled.<span class="fn">fit</span>(balanced_down.<span class="fn">drop</span>(<span class="str">'Exited'</span>, axis=<span class="num">1</span>).values,
                   balanced_down[<span class="str">'Exited'</span>].values)
<span class="cm"># f1_downsampled ‚Üí 0.5986  ‚úì</span></pre>
        </div>
        <div class="result-table-wrap">
          <table class="result-table">
            <thead>
              <tr><th>Approach</th><th>F1 Score</th><th>ROC-AUC</th><th>Train Size</th><th>Status</th></tr>
            </thead>
            <tbody>
              <tr>
                <td class="td-label badge-best">Class Weights</td>
                <td class="badge-best">0.6381</td>
                <td>0.8595</td>
                <td>6,000</td>
                <td class="badge-pass">PASSES ‚úì</td>
              </tr>
              <tr>
                <td class="td-label">Upsampling</td>
                <td>0.6173</td>
                <td>‚Äî</td>
                <td>~9,600</td>
                <td class="badge-pass">PASSES ‚úì</td>
              </tr>
              <tr>
                <td class="td-label">Downsampling</td>
                <td>0.5986</td>
                <td>‚Äî</td>
                <td>~2,400</td>
                <td class="badge-pass">PASSES ‚úì</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="article-p">
          All three passed the F1 ‚â• 0.59 target. Class weights came out on top ‚Äî and the
          rationale is clear:
        </p>
        <ul class="article-ul">
          <li>Highest F1 score (0.6381) ‚Äî exceeds target by +0.048</li>
          <li>Best ROC-AUC (0.8595) ‚Äî strongest discrimination of the three approaches</li>
          <li>Uses all 6,000 training samples as-is ‚Äî no data loss from resampling</li>
          <li>Preserves the natural class distribution for probability calibration</li>
          <li>Simplest to implement in production ‚Äî one parameter change, no preprocessing overhead</li>
        </ul>
        <p class="article-p">
          GridSearchCV then searched <strong>108 hyperparameter combinations</strong>
          (3 √ó 4 √ó 3 √ó 3 values of <code>n_estimators</code>, <code>max_depth</code>,
          <code>min_samples_split</code>, <code>min_samples_leaf</code>) using 3-fold
          cross-validation:
        </p>
        <div class="code-wrap">
          <div class="code-header">
            <span class="code-dot code-dot-r"></span>
            <span class="code-dot code-dot-y"></span>
            <span class="code-dot code-dot-g"></span>
          </div>
          <pre class="code-block">param_grid = {
    <span class="str">'n_estimators'</span>:      [<span class="num">50</span>, <span class="num">100</span>, <span class="num">150</span>],
    <span class="str">'max_depth'</span>:         [<span class="num">5</span>, <span class="num">10</span>, <span class="num">15</span>, <span class="num">20</span>],
    <span class="str">'min_samples_split'</span>: [<span class="num">2</span>, <span class="num">5</span>, <span class="num">10</span>],
    <span class="str">'min_samples_leaf'</span>:  [<span class="num">1</span>, <span class="num">2</span>, <span class="num">4</span>]
}  <span class="cm"># 3 √ó 4 √ó 3 √ó 3 = 108 combinations</span>

grid_search = <span class="acc">GridSearchCV</span>(
    estimator=<span class="acc">RandomForestClassifier</span>(random_state=<span class="num">42</span>, class_weight=<span class="str">'balanced'</span>),
    param_grid=param_grid,
    scoring=<span class="str">'f1'</span>,
    cv=<span class="num">3</span>,
    n_jobs=-<span class="num">1</span>
)
grid_search.<span class="fn">fit</span>(X_train_scaled, y_train)

<span class="fn">print</span>(grid_search.best_params_)
<span class="cm"># {'max_depth': 10, 'min_samples_leaf': 2,</span>
<span class="cm">#  'min_samples_split': 5, 'n_estimators': 100}</span></pre>
        </div>
      </div>
    </div>

    <hr class="article-divider" />

    <!-- FINAL RESULTS -->
    <div class="article-section reveal">
      <h2 class="article-h2">Final Test Results</h2>
      <p class="article-p">
        The model had not seen these 2,000 test samples at any point ‚Äî no training, no validation,
        no tuning. That's what makes this result meaningful: an unbiased estimate of real-world
        performance. Random Forest with <code>class_weight='balanced'</code> was the selected model:
      </p>
      <div class="code-wrap">
        <div class="code-header">
          <span class="code-dot code-dot-r"></span>
          <span class="code-dot code-dot-y"></span>
          <span class="code-dot code-dot-g"></span>
        </div>
        <pre class="code-block">y_test_pred  = rf_weighted.<span class="fn">predict</span>(X_test_scaled)
y_test_proba = rf_weighted.<span class="fn">predict_proba</span>(X_test_scaled)[:, <span class="num">1</span>]

final_f1      = <span class="fn">f1_score</span>(y_test, y_test_pred)          <span class="cm"># 0.6197</span>
final_roc_auc = <span class="fn">roc_auc_score</span>(y_test, y_test_proba)    <span class="cm"># 0.8618</span>

<span class="fn">print</span>(<span class="fn">classification_report</span>(y_test, y_test_pred,
      target_names=[<span class="str">'Stayed'</span>, <span class="str">'Churned'</span>]))
<span class="cm">#               precision  recall  f1-score  support</span>
<span class="cm"># Stayed            0.93    0.87      0.90     1593</span>
<span class="cm"># Churned           0.62    0.77      0.62      407</span></pre>
      </div>
      <div class="result-table-wrap">
        <table class="result-table">
          <thead>
            <tr><th>Metric</th><th>Score</th><th>Notes</th></tr>
          </thead>
          <tbody>
            <tr>
              <td class="td-label">F1 Score (Test)</td>
              <td class="badge-best">0.6197</td>
              <td class="badge-pass">Target ‚â• 0.59 ‚Äî PASSED ‚úì</td>
            </tr>
            <tr>
              <td class="td-label">ROC-AUC (Test)</td>
              <td class="badge-best">0.8618</td>
              <td class="badge-note">Excellent discrimination</td>
            </tr>
            <tr>
              <td class="td-label">F1 Score (Validation)</td>
              <td>0.6381</td>
              <td class="badge-note">Generalization gap: ‚àí0.018</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p class="article-p">
        The gap between validation F1 (0.6381) and test F1 (0.6197) is less than 0.02 ‚Äî
        the model generalizes cleanly and isn't overfitting. A ROC-AUC of 0.8618 means the
        model correctly ranks 86% of churn-vs-stay customer pairs. It genuinely knows the
        difference.
      </p>
    </div>

    <hr class="article-divider" />

    <!-- MAIN TAKEAWAYS -->
    <div class="article-section reveal">
      <h2 class="article-h2">Main Takeaways</h2>
      <ul class="article-ul">
        <li><strong>Class imbalance correction is non-negotiable.</strong> Baseline Random Forest scored 0.5573. With class weights: 0.6381. That single parameter change was the difference between failing and exceeding the target.</li>
        <li><strong>Class weights are the most efficient fix.</strong> No data manipulation, no information loss, no resampling overhead ‚Äî just a weight adjustment that shifts the model's focus.</li>
        <li><strong>Always test missing data before imputing.</strong> The MAR test confirmed that filling Tenure with the median wouldn't introduce systematic bias ‚Äî a 5-minute test that validated the whole approach.</li>
        <li><strong>F1 score is the right target for imbalanced problems.</strong> Accuracy would have been misleading ‚Äî a model that predicted "stayed" for everyone would score 79.6% while catching zero churners.</li>
        <li><strong>Hyperparameter tuning is a search, not a guess.</strong> GridSearchCV found that shallower trees with conservative leaf settings outperformed deeper, more complex configurations.</li>
      </ul>
    </div>

    <hr class="article-divider" />

    <!-- GROWTH CALLOUT ‚Äî churn article -->
    <div class="article-section reveal">
      <div class="growth-block">
        <p class="growth-block-label">What I Learned &amp; Why It Matters to Employers</p>
        <p>
          Sprint 9 was where I moved from running models to <em>understanding</em> them. The
          difference between a 0.3190 and 0.6381 F1 score came down to one parameter ‚Äî
          <code>class_weight='balanced'</code> ‚Äî but knowing <em>why</em> that mattered required
          understanding class imbalance, metric selection, and what the model was actually
          optimizing for. I also ran a formal MAR test before imputing missing data, and used
          GridSearchCV to search 108 hyperparameter combinations systematically rather than
          guessing. These aren't shortcuts ‚Äî they're the habits that separate exploratory ML
          from production-ready thinking.
        </p>
      </div>
    </div>

    <!-- CONCLUSION -->
    <div class="article-section reveal">
      <h2 class="article-h2">Conclusion &amp; Reflections</h2>
      <p class="article-p">
        This project reinforced something I believe deeply: good data science isn't just about
        algorithms ‚Äî it's about asking the right questions. Why are values missing? Why is the
        model underperforming? Which metric actually reflects the business goal?
      </p>
      <p class="article-p">
        In a real deployment, a model like this could run inside Beta Bank's CRM every week ‚Äî
        generating a prioritized list of at-risk customers for the retention team. Some percentage
        of those customers get a call, a better rate, or an offer, and they stay. That's
        measurable business value, built in days rather than months.
      </p>
      <p class="article-p">
        The hardest moment? Seeing Logistic Regression post an F1 of 0.3190 on the first run.
        It's discouraging. But that's the job ‚Äî diagnose the problem (class imbalance), apply the
        fix (class weights), measure the improvement, move forward. F1 went from 0.3190 to 0.6381.
        That's the process working exactly as it should.
      </p>
      <div class="result-table-wrap">
        <table class="result-table">
          <thead>
            <tr><th>Project Requirement</th><th>Status</th></tr>
          </thead>
          <tbody>
            <tr><td class="td-label">F1 Score ‚â• 0.59 on test set</td><td class="badge-pass">ACHIEVED ‚Äî 0.6197 (+0.030 margin) ‚úì</td></tr>
            <tr><td class="td-label">At least 2 imbalance techniques tested</td><td class="badge-pass">YES ‚Äî 3 tested (class weights, upsampling, downsampling) ‚úì</td></tr>
            <tr><td class="td-label">Proper train / validation / test split</td><td class="badge-pass">YES ‚Äî stratified 60/20/20 ‚úì</td></tr>
            <tr><td class="td-label">ROC-AUC examined</td><td class="badge-pass">YES ‚Äî 0.8618 on test set ‚úì</td></tr>
            <tr><td class="td-label">Data preprocessing completed</td><td class="badge-pass">YES ‚Äî MAR test, median imputation, encoding ‚úì</td></tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- CALL TO ACTION -->
    <div class="article-cta">
      <h3>Want to Explore the Full Code?</h3>
      <p>The complete notebook ‚Äî all 5 phases, every model, every result ‚Äî is on GitHub.</p>
      <div class="cta-buttons">
        <a href="https://github.com/viet-in-tech/tripleten-sprint-9-feature-engineering"
           class="btn btn-primary" target="_blank" rel="noopener noreferrer">
          View Notebook on GitHub ‚Üí
        </a>
        <a href="https://linkedin.com/in/vietnguyenstem"
           class="btn btn-outline" target="_blank" rel="noopener noreferrer">
          Connect on LinkedIn
        </a>
        <a href="index.html#projects" class="btn btn-outline">‚Üê Back to Projects</a>
      </div>
    </div>

  </main>

  <!-- ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <footer class="footer">
    <div class="container">
      <p>Designed &amp; built by <span class="accent">Viet Nguyen</span></p>
      <p class="footer-sub">Built with HTML, CSS &amp; JavaScript ¬∑ 2026 ¬∑ Assisted by <a href="https://claude.ai/claude-code" target="_blank" rel="noopener noreferrer">Claude Code</a></p>
      <p class="footer-hint">‚Üë‚Üë‚Üì‚Üì‚Üê‚Üí‚Üê‚ÜíBA</p>
    </div>
  </footer>

  <script src="script.js"></script>
</body>
</html>
